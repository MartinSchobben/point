---
title: "IC-introduction"
author: "Martin Schobben"
output: 
  bookdown::html_document2:
    toc: true
bibliography: SIMS.bib
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{IC-introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
  
# Introduction to point

The large imprecision in secondary ion generation during during, e.g, Secondary Ion Mass Spectrometry (SIMS), and associated registration of ion counts with the detection devices, requires the generation of large quantities of data in order to suppress the uncertainty associated with the final output value. This would be the case either with single ion abundances or isotope ratios, and causes the accumulation of large datasets and numerous files. Additional meta-data files record machine-specific settings, e.g., regarding the optics, beam stability and mass spectrometer of e.g., a *Cameca NanoSIMS 50L* in my case, that vary depending on the type of analyses or environmental conditions. And, although, the default software of the instrument (e.g. Cameca software) can do all of this data accumulation and transformation, there are certain benefits in getting more control on those actions, which are hidden away in the code of the commercial software. Foremost, it enables controlling ion count corrections related to systematic biases which are linked to the detection systems. Furthermore, more control on the data analysis allows sub-setting or collating of analyses to calculate either internal or external precision of the study. Perhaps the most powerful aspect of the greater freedom associated with the usage of point is the performance of diagnostics that evaluate the internal consistency of the ion count ratios. These diagnostics can inform about factors that are normally not easily detectable, such as, heterogeneity of the sampled surface or instability of the instrument during individual runs. All of these features are in discussed in detail in the accompanying vignettes, whereas this vignette will only delve into the basics of the workflow.

## Nomenclature
Concise terminology is required when discussing pulsed ion count data to prevent ambiguity surrounding for instance the usage of terms such as the analytical substrate (the rock or biological sample), machine setup and instrument performance as well as conventions associated with statistics. It would become easy to mix-up terms, as, for instance, sample can refer to the physical sample analysed during a study, but sample can also relate to the set of individual objects collected during a statistical analysis, as such we refer henceforward to analytical substrate when discussing the former. The levels at which ion counts are detected and accumulated during a study can be another source of confusion, where we have the single instance of an ion hitting the detector, or the event. However, in order to get an idea of the abundance of a certain chemical species, we need of course a suite of these events which then together constitute a single measurement. So, the measurement ($N_i$) would the minimal level at which the machine starts recording and generating data output. Due to the fundamental imprecision of pulsed ion count measurements, it is necessary to record a whole collection, or series, of these measurements in order to limit the uncertainty surrounding our abundance estimate of the chemical species. In other words, we want to know how well our sample (i.e., the statistical sample) approximates the true population mean ($\mu$). This accumulation of measurements, or $n$-series of measurements, is then collectively referred to as an analysis (or $N_{(i)} = M_j$). A study, on for instance the carbon isotope composition of a carbonate rock, would not rely on only one such analysis but would compare a suite of analyses across its surface, or even between different specimens. This is then referred to as a study, or $m$-series of analyses $M_{(j)}$.

To summarise:

* Sample: sample of the true population
* Analytical substrate: physical sample measured during SIMS analysis
* Event: single event of an ion hitting the detector
* Measurement: single count cycle $N_i$
* Analysis: $n$-series of measurements $N_{(i)} = M_j$ 
* Study: $m$-series of analyses $M_{(j)}$, constituting the different spots on the analytical substrate  

This terminology has been used throughout the documentation of the package point, and the above summary has been included in each of the vignettes for convenience. 

```{r setup}
library(point) # load package
```


## The point workflow

The most basic workflow consists of three steps:

1) Loading the machine generated raw ion counts and collating them into a dataframe ([tibble](https://tibble.tidyverse.org/)) with the function `read_IC`.
2) Correcting the raw ion count data for systematic biases related to the detection device with the function `cor_IC`.
3) Calculating statistics that inform about the impact of random biases on the precision of the analysis or study with the function `stat_Xt` for single ions and `stat_R` for isotope ratios. 

A more specialised step includes the performance of an internal consistency check on variation among individual variations, and the adoption of a model that can account for this. The generated diagnostics allow filtering of the original dataset with e.g., `dplyr::filter` based on a flag, and subsequent recalculation of the precision with `stat_*`. This augmentation/optimization procedure can help detect machine instability or heterogeneity of the analytical substrate. 

```{r workflow-graph, out.width="100%", echo=FALSE}

knitr::include_graphics("workflow.png")

```

For more detailed information:

[IC-read](IC-read.html): reading raw ion count data use    
[IC-process](IC-process.html): processing ion count data        
[IC-precision](IC-precision.html): statistics concerning ion count precision    
[IC-diagnostics](IC-diagnostics.html): diagnostics on internal variation
