---
title: "IC-diagnostics"
author: "Martin Schobben" 
output: 
  bookdown::html_document2:
    toc: true
bibliography: SIMS.bib
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{IC-diagnostics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Diagnostics for Ion Count Data
The random nature of secondary ions emitted from a sample is described by Poisson statistics, which can be used to predict the precision of SIMS measurements under ideal circumstances (e.g., the predicted standard error can be deduced from the total counts of secondary ions). However, besides this fundamental source of imprecision, real SIMS measurements are additionally affected by other factors such as sample heterogeneity, instrument instability, the development and geometry of the sputter pit, and sample charging. Although some of these biases can be avoided by proper instrument tuning and sample documentation (e.g. T/SEM to characterise the textural properties of a rock sample) prior to SIMS measurement, factors such as instrument instability or sample heterogeneity can never be fully eliminated. In this vignette, diagnostic tools are showcased which can help evaluate the potential impact of such factors on the precision of ion count data.

```{r setup}
library(point)
devtools::load_all(".")
```

The following packages are used in the examples that follow.

```{r additiona_ packages, message = FALSE}
library(purrr) # functional programming
library(dplyr) # manipulating data
library(ggplot2) # graphics
```

```{r plot_defaults, echo=FALSE}
# Default ggplot theme
theme_set(theme_classic())
theme_replace(axis.title = element_text(size = 11),
              axis.text = element_text(size = 9),
              plot.background = element_rect(fill = "transparent",
                                             color = NA),
              panel.background = element_rect(fill = "transparent",
                                              color = NA),
              legend.background = element_rect(fill = "transparent",
                                               color = NA)
              )
```

## Nomenclature

* Sample: sample of the true population
* Analytical substrate: Physical sample measured during SIMS analysis
* Event: single event of an ion hitting the detector
* Measurement: single count cycle $N_i$
* Analysis: $n$-series of measurements $N_{(i)} = M_j$ 
* Study: $m$-series of analyses $M_{(j)}$, constituting the different spots on the analytical substrate 

## Isotope ratios: A special case
Isotopes of the same element should have more-or-less the same ionization efficiency [@Fitzsimons2000a]. It thus follows, that in an isotopically homogeneous analytical substrate the count rates of two isotope from the same element should be dependent on each. From this it can be deduced that large deviations in count rate ratios of an isotope system between consecutive measurements indicate a potential discrepancy (e.g., sample heterogeneity and instrument instability).  

### Count block based diagnostics (Default Cameca software procedure)
The default method to account for these discrepancies as incorporated in the Cameca software entails a blockwise check for variability. If values fall outside of a pre-defined range of variance (for example two standard deviations; named $\sigma$ in the Cameca software), the measurement will be rejected.  


```{r warning=FALSE}

# Use point_example() to access the examples bundled with this package 

# Carry-out the routine point workflow
# Raw data containing 13C and 12C counts on carbonate
tb.rw <- read_IC(point_example("2018-01-19-GLENDON"))

# Processing raw ion count data
tb.pr <- cor_IC(tb.rw, 
                N = N.rw, 
                t = t.rw, 
                Det = det_type.mt, 
                deadtime = 0, 
                thr_PHD = 0)

# CAMECA style augmented datafile
# Vectors of isotope ratios 
ion1 <-  c("13C", "12C 13C", "13C 14N", "12C") 
ion2 <-  c("12C", "12C2", "12C 14N", "40Ca 16O")

# Call function Diag_R over all ion ratio combinations  
tb.aug <- purrr::map2(ion1, ion2, ~(diag_R(tb.pr,
                                           method = "Cameca",
                                           args = expr_R(Xt = "Xt.pr",
                                                         N = "N.pr",
                                                         species = "species.nm",
                                                         ion1 = .x,
                                                         ion2 = .y
                                                         ),
                                           file.nm,
                                           bl.mt
                                           ) %>%
                                      filter(flag == "non-influential")
                                    )
                      )
```


```{r echo=FALSE}                                    
# Reproduce Cameca stat file: descriptive an predictive statistics for  
# ratios (blockwise)

# List variables
ls.CAMECA <- lst(df = tb.aug, a = ion1, b = ion2)

# Function to transform to calculate block wise and produce CAMECA output style
fun_CAMECA <-function(df, a, b){
  
               df <- stat_R(df, Xt.pr, N.pr, species.nm, 
                            a, 
                            b, 
                            file.nm, 
                            bl.mt,
                            output = "complete"
                            ) %>%   
                       filter(file.nm == "2018-01-19-GLENDON_1_1") %>% 
                       distinct(bl.mt, .keep_all = TRUE) %>% 
                       mutate_at(vars(contains("meas_bl.mt")), ~(. - n_R_Xt.pr)) %>% 
                       mutate(`Err_mean (%)` = RSeM_R_Xt.pr / 10,
                              `Poisson (%)` = hat_RSeM_R_Xt.pr / 10,
                              `Ratio#` = paste(a, b, sep = "/")
                              ) %>% 
                       select(`Block#` = bl.mt, 
                              `Ratio#` , 
                              "Mean" = M_R_Xt.pr, 
                              "SD" = S_R_Xt.pr, 
                              N_rej = contains("meas_bl.mt")[1], 
                              `Err_mean (%)`,
                              `Poisson (%)`,
                              "Khi2" = chi2_R_Xt.pr
                       )
               }

# Call the function
tb.CAMECA <- purrr::pmap_dfr(ls.CAMECA, fun_CAMECA) %>% 
              arrange(`Block#`)
              
```


```{r xcpcameca, out.width="90%", echo=FALSE, fig.cap="An excerpt of the Cameca stat-file for count block based 2$\\sigma$-rejection and associated blockwise statistics"}

knitr::include_graphics("excerpt_stat.png")
```


```{r echo=FALSE}
knitr::kable(tb.CAMECA[1:13, ],
             format.args = list(digits = 2, 
                                format = "G", 
                                flag = "0"),
              caption = "Blockwise statistics for augmented dataset (replication of Fig. \\@ref(fig:xcpcameca)).") 
```

Note, that in the above example of the Cameca diagnostics (Fig. \@ref(fig:xcpcameca)), ratios of none-isotope pairs have been calculated.

```{r ion, fig.width=8, fig.height=6, fig.cap="The blockwise mean plotted against consecutive blocks. Note that the non-isotope ratios show a decreasing trend over time.", echo=FALSE}

ggplot(tb.CAMECA, aes(x =`Block#`, y = Mean)) +
  geom_point() +
  facet_wrap(vars(`Ratio#`), scales = "free")
```


Upon plotting the blockwise mean against the block number (Fig. \@ref(fig:ion)), it becomes apparent that the none-isotope ratios display a monotonic decreasing trend over the duration of the analysis. This effect likely reflects the differential ionization potentials and trajectories of secondary beam stabilization over the analysis (as explained above).

Based on the Cameca $\sigma$-rejection, it is possible to augment the dataset for each of the analyses by simply removing the anomalous measurements ($N_i$), which pertains to the $N\_rej$ in Fig. \@ref(fig:xcpcameca).   

````{r echo=FALSE}
# Augmented descriptive an predictive statistics (global dataset)

# Function to transform to calculate block wise and produce CAMECA output style
fun_CAMECA_gl <-function(df, a, b){
  
               df <- stat_R(df, Xt.pr, N.pr, species.nm, 
                                            a, b, file.nm, 
                                            output = "complete") %>%   
                       filter(file.nm == "2018-01-19-GLENDON_1_1") %>%
                       
                       mutate(`Err_mean (%)` = RSeM_R_Xt.pr / 10,
                              `Poisson (%)` = hat_RSeM_R_Xt.pr / 10,
                              `Ratio#` = paste(a, b, sep = "/"),
                              SD_bl = sd(R_Xt.pr) /
                                      mean(R_Xt.pr) * 100) %>% 
                       distinct(file.nm, .keep_all = TRUE) %>% 
                       select(`Ratio#` , 
                              "Ratios" = M_R_Xt.pr, 
                              `Poisson (%)`,
                              `Err_mean (%)`,
                              "Khi2" = chi2_R_Xt.pr,
                              `SD_Block(%)`= SD_bl)
}

# Call the function
tb.CAMECA_aug <- purrr::pmap_dfr(ls.CAMECA, fun_CAMECA_gl)

# And the same without augmentation of the dataset
ls.CAMECA$df <- replicate(length(ion1), tb.pr, simplify = FALSE)

tb.CAMECA_org <- purrr::pmap_dfr(ls.CAMECA, fun_CAMECA_gl)

```

```{r xcpcameca2, out.width="90%", echo=FALSE, fig.cap="An excerpt of the Cameca stat-file for count block based 2$\\sigma$-rejection and associated blockwise statistics"}

knitr::include_graphics("excerpt_stat_global.png")
```

```{r aug, echo=FALSE}
knitr::kable(tb.CAMECA_aug,
             format.args = list(digits = 2, 
                                format = "G", 
                                flag = "0"),
             caption = "Summary stats for augmented dataset"
             ) 
```


```{r org,echo=FALSE}
knitr::kable(tb.CAMECA_org,
             format.args = list(digits = 2, 
                                format = "G", 
                                flag = "0"),
             caption = "Summary stats for original dataset"
             ) 
```

However, when comparing the summary statistics of the augmented dataset (Table \@ref(tab:aug)) with the excerpt from the Cameca stat-file (Fig. \@ref(fig:xcpcameca2)) their seems to be a substantial difference. Instead upon comparison with Table \@ref(tab:org), which consist of summary statistics calculated on the original dataset (without $\sigma$-rejection of measurements), the replication is surprisingly consistent. This heralds the question whether there might be a mistake in the Cameca software for calculating statistics on ion ratios for complete analyses. Otherwise the blockwise $\sigma$-rejection of measurements seems rather obfuscated, as it does not seem to serve an obvious purpose.

### Regression diagnostics

Precision monitoring by combined predictive and descriptive statistical analysis informs on errors that reflect on a combination of the random nature of secondary ion generation, purely analytical biases and artifacts resulting from the homogeneity of the analytical substrate. But what now if the analysed material contains isotopic heterogeneity at a smaller scale than the beam size, could this be detected? 

The solution seems straightforward, just compare the $R$ values of the individual measurements of an $n$-series analysis ($N_{(i)}$), and see whether there are significant deviations at some points during the analysis. However, the situation is more complex than that, as illustrated with the following examples. For these examples, the simulated count rates for three hypothetical types of materials are generated (Fig. \@ref(fig:simulations)). 

1) The **"ideal" substrate** with a completely homogenous composition  
2) A **substrate with a "gradient"** which has a gradient spanning the whole depth covered by the analysis 
3) A **substrate with a "constant" offset** which has a sudden offset in isotope composition at a specific depth  

In the two simulations with heterogenous isotope composition the total span is set to 60‰, which is a large variance for most natarul isotope systems. A linear trend has been superimposed on the count rates of both isotope species, which is an often encountered phenomenan and relates to changes in the ionization potential, and, as discussed before, this effect afflicts both isotope species to a similar degree thereby not changing the $R$ [see @Fitzsimons2000a]. 


```{r simulations, echo=FALSE, fig.width=12, fig.height=6, fig.cap="Histograms comparing input and output $R$ values for simulations of an ideal isotopically homogenous material, a gradual gradient in $R$ (gradient), and a sudden shift in $R$ (constant)"}

# repetition
reps <-50

# Types of R variation
var_R <- c("ideal", "constant", "gradient")

# Varying linear trends in the ionization efficieny
var_T <- seq(0, 0.5, length.out = 11)

# seeds for number generation
tot_length <- length(var_T) * length(var_R)
var_seed <- 1:tot_length

sim_R.ext <- tidyr::crossing(sys = var_T, type = var_R) %>% 
  mutate(seed = var_seed) %>% 
  transmute(params = purrr::pmap(lst(sys, type, seed), lst))


sim_R.ext <- purrr::map2_dfr(rep("sim_R", tot_length) %>% set_names(nm = paste("run", 1:tot_length)), 
         sim_R.ext$params, 
         function(fn, args) exec(fn, 
                                 !!! args,
                                 reps = reps,
                                 ion1 = "13C", 
                                 ion2 = "12C", 
                                 baseR = 5, 
                                 offsetR = -55
                                 ),
         .id = "run") %>%
  tidyr::separate(simulation, sep = "-", c("simulation", "repetition"))

# limited dataset
sim_R <- sim_R.ext %>% 
  filter(trend == "linear trend (var: 0)" | 
         trend == "linear trend (var: 0.5)",
         repetition == 1   
         )

# Compare input value of R with output 
tb.R <- sim_R %>% 
  stat_R(Xt.sim, N.sim, species, "13C", "12C", simulation, trend, output = "complete") 

com.R <- tb.R %>% 
  select(simulation, trend, R.input = R.input.13C, R.output = R_Xt.sim) %>% 
  tidyr::pivot_longer(c(R.input, R.output), names_to = "source", values_to = "R")

ggplot(com.R, aes(x = R, fill = source)) +
  geom_histogram(aes(y =  stat(ncount)), position = "identity", alpha = 0.4, binwidth = 0.0001) +
  facet_grid(cols = vars(simulation), rows = vars(trend))

```

It becomes evident from Figure \@ref(fig:simulations) that the real variation in $R$ (input R; red) is indistinguisable in the $R$ derived from the ion counts (output R; blue). This is an effect of the relative large random variation induced by the ionization. So, even for these large variations in isotopic composition, it cannot be confidently said whether the measured $R$ is representative for the analysed site, or just some averaged value of an otherwise isoptically variable substrate. This effect is, furthermore, hardly observable in the precision of the data, where the $\epsilon_R$ of the simulation dubbed *constant* is `r tb.R %>% filter(simulation == "constant", trend == "linear trend (var: 0.1)") %>% pull(RSeM_R_Xt.sim) %>% unique() %>% round(2)`‰, whereas the *ideal* simulation has only a marginally better precision of `r tb.R %>% filter(simulation == "ideal", trend == "linear trend (var: 0.1)") %>% pull(RSeM_R_Xt.sim) %>% unique() %>% round(2)`‰. This univariate approach will, however, distinguish these analyses based on the mean (*constant* simulation: `r tb.R %>% filter(simulation == "constant", trend == "linear trend (var: 0.1)") %>% pull(M_R_Xt.sim) %>% unique() %>% calib_R(standard = "VPDB", type = "composition", input = "R", output = "delta") %>% round(2)`‰ and *ideal* simulation: `r tb.R %>% filter(simulation == "ideal", trend == "linear trend (var: 0.1)") %>% pull(M_R_Xt.sim) %>% unique() %>% calib_R(standard = "VPDB", type = "composition", input = "R", output = "delta") %>% round(2)`‰), even though, an average values is not really representative for the former case as the original population of $R$ is highly variable and skewed. So, clearly, a different approach must be adopted to identify these isopically highly variable substrates.


<!--  From the previous examples it becomes apparent that even largeky heterogenous substrates cause only minimial deviation in the internal precision. This could be interperted that their is perhaps a lot of hidden variation in the nominal SIMS isotope analysis, which skews $\bar{R}$ but leaves precision untouched. -->

#### Isotope data viewed as bivariate ion counts

In an "ideal" isotopically homogeneous analytical substrate the count rates of two isotope from the same element should be dependent on each other. Thus, even though the ionization efficiency might vary within a single analysis, the count rate of an isotope species $b$ can be deduced from the count rate of isotope species $a$ through a linear combination with the isotope ratio $R$:

\begin{equation}
  E(X_i^b|X_i^a) = RX_i^a \;\;\; \text{where} \;\;\; (X_i^b|X_i^a) \sim P(\lambda)
  (\#eq:conR)
\end{equation}

Variation in this linear count rate relationship is then only caused by the random nature of secondary ion generation, which follows a Poisson distribution. If, however, $N_i$ is sufficiently larger for both isotope species, the previous linear combination (Eq. \@ref(eq:conR)) should approximate: 

\begin{equation}
  E(y_i|x_i) = \beta_0 + \beta_1 x_i + \epsilon_i \;\;\; \text{where} \;\;\; \epsilon_i \sim N(0,\sigma^2) \;\;\; \text{with} \;\;\; E(\epsilon_i|x_i) = 0
    (\#eq:conmean)
\end{equation}

where, a vector of independent values $x$ and a vector of coefficients ($\beta$) can be used to express the conditional mean of $y$ at any $i$. This formulation is also known as the population linear regression function. Instead of using Ordinary Least Squares (OLS) to obtain estimates for the regression coefficients, the $\bar{R}$ can be used as an approximation of $\beta$. Substituting Eq. \@ref(eq:conR) in Eq. \@ref(eq:conmean) further suggests that $\hat{\beta}_0$ should then approximate:

\begin{equation}
  \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} = 0
  (\#eq:estB0)
\end{equation}

Adopting this linear combination of parameters, the ideal model for isotope count data is as follows; 

\begin{equation}
 \hat{X}_i^b = \bar{R}X_i^a +  \epsilon_i \;\;\; \text{where} \;\;\; \epsilon_i \sim N(0,\sigma^2)  \;\;\; \text{if} \;\;\; X_i^a, X_i^b \gg 0
  (\#eq:linmodR)
\end{equation}

And, thus the following measure can be adopted to assess the performance of this "ideal" linear model: 

\begin{equation}
 \hat{\epsilon}_i =  X_i^b - \bar{R}X_i^a  
  (\#eq:resR)
\end{equation}


```{r regression, echo=FALSE,  fig.cap = "Cross plotted ^13^C and ^12^C count rates and the regression line following equation, based on the linear function of Eq. \\@ref(eq:linmodR) and with a 2 times the standard error of the regression represented by the green area.", fig.width=12, fig.height=6}


# diagnostics and plots of simulations
gg.sim.ex <- plot_RDiag(sim_R, Xt.sim, N.sim, species, "13C", "12C", simulation, trend)

gg.sim.ex[[1]] 
```


The simulated scenarios in Fig. \@ref(fig:regression) visualise the comparison of the model following Eq. \@ref(eq:linmodR) and hypothetical bivariate ^13^C-^12^C ion count rates. A greater random spread about the model can already be observed in the simulations of isotopically heterogenous substrates. 

The acquired $\hat{\epsilon}_i$ can be used to validate three important assumptions of the previously adopted model (Eq. \@ref(eq:linmodR)):

1) $\hat{\epsilon}_i$ should vary independtly from the predicted value (e.g. constant variance)
2) $\hat{\epsilon}_i$ should be approximately normally distributed with a mean of zero (e.g. normality of residuals)
3) $\hat{\epsilon}_i$ should be independent random variables over time (e.g. independence of residuals)

To over come scale dependence of the residuals, the residuals will first be normalized. This can be done by using the *studentized* residuals ($\hat{\epsilon}_{i}^*$). A studentized residual is obtained by dividing the residual by an independent estimate of the standard deviation of the  residuals. To ensure independence, the standard deviation of the residuals is calculated by leaving the residual of the $i$-th observation out:

\begin{equation}
  \hat{\epsilon}_i^* = \frac{\hat{\epsilon}_i}{S_{\hat{\epsilon}_{(-i)}} \sqrt(1-h_i)}
  (\#eq:studres)
\end{equation}

, where the standard error of the regression (standard deviation of the residuals) is calculated by leaving the $i$-th residual out:

\begin{equation}
  S_{\hat{\epsilon}_{(-i)}} = \sqrt{\frac{1}{n-k-1} \sum_{i = 1}^{n}\hat{\epsilon}_i^2}
  (\#eq:SEminusi)
\end{equation}  
  
In Eq. \@ref(eq:studres) $h_i$ represents the leverage of the indepedent variable ($X^a$), or *hat-value*. The hat-value measures the distance from the mean of the independent variable $X_i^a$:

\begin{equation}
 h_i = \frac{1}{n} + \frac{ \left( X_i^a - \bar{X}_a \right)^2 }{\sum_{j=1}^n \left(X_j^a -  \bar{X}_a \right)^2}
 (\#eq:hatvalue)
\end{equation}

The further $X_i^a$ from the mean the more leverage the data-point has on the regression line.

**Assumption 1: Constant variance**

The studentized residuals provide a more effective means of detecting the outliers and provides a means to validate the first assumption of equal variance. This has been graphically visualized in a *scale location plot* (Fig. \@ref(fig:ScLoc)) of $\hat{\epsilon}_i^*$, where values of above 3.5 can be considered as outliers ("the green band").


```{r ScLoc, echo=FALSE, fig.cap = "Scale-Location plot, can be used to check for systematic relationships between $\\hat{\\epsilon}_i^*$ and the independent value ^13^C (Heteroscedasticity) or the homogeneity of variance (Homoscedasticity). Blue dots demarcate data-points that can be considered outliers based on $\\hat{\\epsilon}_i^*$  < -3.5 and 3.5 > $\\hat{\\epsilon}_i^*$. The dashed line and green area denotes the value and spread for studentized residuals if Homoscedasticity exists.",  fig.width=8, fig.height=4}

# Filtering ideal simulation for model validation
sim_R.ideal <-sim_R %>% filter(simulation == "ideal")

# diagnostics and plots of simulations
gg.sim <- plot_RDiag(sim_R.ideal, Xt.sim, N.sim, species, "13C", "12C", simulation, trend)

gg.sim[[4]] 

```

Patterns in the homogeneity of variance on the *scale-location* plot can test whether the linear model suffers from systematic variations in the residuals that are dependent on the dependent variable ($X^b$), termed heteroscedasticity. When an heteroscedastic error occurs the estimates of coefficient can be compromised, and might suggest that a linear model does not fit the data. By plotting a linear least square model on the location-scale plots the systematic relation between $\hat{Y}_i$ and $\hat{\epsilon}_i$ (Eq. \@ref(eq:conmean)) can be further analysed.  This method can be further formalized with a hypothesis test known as the *Breusch Pagan test*. 

```{r bptest, echo=FALSE, fig.cap="Breusch Pagan $\\chi^2$ values for the ideal simulation for several linear trends in the ionization efficiency.", fig.width=6, fig.height=4}

Chi_R2 <- diag_R(sim_R.ext %>% filter(simulation == "ideal",
                                      repetition == 1 
                                      ), 
                 method = "CooksD", 
                 args = expr_R(Xt = "Xt.sim", 
                               N = "N.sim", 
                               species = "species", 
                               ion1 = "13C", 
                               ion2 = "12C"
                               ),
                 simulation, 
                 trend,
                 output = "complete"
                 ) %>% 
  # select(simulation, trend, `variance of residuals` = flag_CV, Chi_R2) %>%  
  distinct(trend, .keep_all = TRUE)

df <- 1
upper <- 10
# curve
df_den <- tibble::tibble(vec_den = seq(0.1, upper))

#find upper  95% of distribution
upper95 <- qchisq(.95, df)


ggplot(df_den, aes(x=vec_den)) +
  stat_function(fun = dchisq, args = list(df = df)) +
  stat_function(fun = dchisq, 
                      args = list(df = df),
                      xlim = c(upper95, upper),
                      geom = "area",
                      alpha = 0.2) +
  geom_vline(xintercept = upper95 , linetype = 2)+
  geom_point(data = Chi_R2, aes(x = Chi_R2, y = dchisq(Chi_R2, df = df),
                                color = trend
                                )
             ) +
  annotate("text", label = "95% confidence limit", x = upper95 * 1.2, 
           y = 0.5, angle = 90
           ) +
  xlab(expression(chi^2~"(df =1)")) +
  ylab("probability-density")

```

The output of this test has a $\chi^2$-distribution of 1 degrees of freedom so that a 95% confidence level for $H0$ rejection can be choosen from the $\chi^2$-distribution (see Fig. \@ref(fig:bptest)). Performing this test on the studentized residuals and fitted values suggests homogeneity of variance for the ideal model for most situations, unless when no linear trend in ionization efficiency is evident. However, the correlation between $\chi^2$ probability and the linear ionization trend seems spurious, and is likely an artefact related to the relative large uncertainty in the OLS estimate which is an effect of the small variation in the independent variable ($X^a$)(see Fig. \@ref(fig:coeftest)). So, for the ion count originating from the *ideal* simulation, it can be concluded that there no strong reason to reject the $H0$ of homoscedasticity, and the assumption of **constant variance** holds. 

```{r coeftest, echo=FALSE, fig.cap="Comparison of the standard error of the $\\beta_1$ coefficient of an OLS for estimating the studentized residuals and the variation in the dependent variable $S[X^b]$ for several linear trends in the ionization efficiency.", fig.width=6, fig.height=4}

ggplot(Chi_R2, aes(y = SE_beta , x = S_Xt.sim.12C, color = trend)) +
  geom_point() +
  xlab(expression(S[""^12*C])) +
  ylab(expression(S[beta[1]]))

```


<!-- In the future I will test what the lower limit of n is so that this assumption is violated -->

**Assumption 2: Normality of residuals**

A second test validates the assumption of normality of the $\hat{\epsilon}_i^*$. This can be achieved by means of a Quantile-Quantile comparison plot (QQ normality plot; Fig. \@ref(fig:QQnorm)), where the empirical sample distribution is compared with a theoretical distribution. The theoretical cumulative distribution function $P(x)$ (or CDF) is used to compare the empirical sample distribution with, so that $Pr(X \le x) = P(x)$. To achieve this the studentized residuals are ordered from smallest to largest, denoted $\hat{\epsilon}_1^*$, $\hat{\epsilon}_2^*$, ..., $\hat{\epsilon}_i^*$. The cumulative probabilites of the empirical sample distribution for the $i$-th follows:

\begin{equation}
  P_i = \frac{i - \frac{1}{2}}{n} 
  (\#eq:probs)
\end{equation}  

And by inverting the CDF we can find the theoretical quantiles $z_i$ (from $\mathrm{N}(0,1)$) that correspond with the cumulative probability, by using:

\begin{equation}
  z_i = P^{-1} \left( \frac{i - \frac{1}{2}}{n} \right)
  (\#eq:quant)
\end{equation}

The obtained $z_i$ values are plotted as horizontal coordinates against the ordered $\hat{\epsilon}_i^*$ values in Figure \@ref(fig:QQnorm) (QQ normality plot). Here $P^{-1}$ is the inverse CDF. The straight line in the plot is a model estimate for $\hat{\epsilon}_i^*$ for comparison because if the residuals were a normal sample, the observations should lie on a straight line:

\begin{equation}
 \hat{\eta}_i = \hat{\mu} + \hat{\sigma}z_i 
 (\#eq:predQQ) 
\end{equation}

, where $\hat{\mu}$ and $\hat{\sigma}$ can be calculated from the mean and standard deviation of $\hat{\epsilon}_i^*$. As there is an expected departure from this line because of sampling variation, a standard error for the order statistic $\eta_{i}$ is required:

\begin{equation}
  S_{\eta_i} = \frac{\hat{\sigma}}{\Phi(z_i)} \times \sqrt{\frac{P_i(1- P_i)}{n}} 
  (\#eq:seQQ)
\end{equation}

where $\Phi$ is a density of $\mathrm{N}(0,1)$, so with the appropriate standard error being:

\begin{equation}
  \hat{\eta}_i \pm 2 \times S_{\eta_i}
  (\#eq:seQQ2)
\end{equation}

Thereby providing a means to distinguish $\hat{\epsilon}_i^*$ that deviate significantly from the normal assumption (Fig. \@ref(fig:QQnorm)). The differences between the emperical and theoretical quantiles can also be formalized as an hypothesis test, where the *Anderson-Darling test* is one example of such test. Based on this test, there is no reason to reject the $H0$ of normality in the case of an "ideal" analysis.    

```{r QQnorm, echo=FALSE, fig.cap = "QQ normality plot, comparing the sample quantiles with the theoretical quantiles of a density distribution with N(0,1). Blue dots denote data-points that fall outside the 95% confidence range of the predicted relationship between the theoretical and sample quantiles", fig.width=8, fig.height=4}

gg.sim[[2]] 
```


One can further deduce from Fig. \@ref(fig:QQnorm)), that the variance around the linear line centers around zero with most data spread in an area encompassing the 95% confidence of this regression line. This can be further confirmed by a two sided one-sample t-test on the studentized result with  $H0$: $\mu_0 = 0$, and  $Ha$: $\mu_0 \neq 0$. The assumption of **normality of residuals** with a conditional mean of zero (Eq. \@ref(eq:linmodR)) seems to be valid for the "ideal" simulation.  

**Assumption 3: Independence of residuals**

Count data from one measurement to another should be truly independent distributions, thus without machine bias and sample heterogeneity the difference between consective measurements (differencing) should be uncorrelated (autocorrelation). This approach can be extended to encompass not only differencing with one lag but also the next and the one thereafter (see Fig. \@ref(fig:acf)).

```{r acf, echo=FALSE, fig.cap = "The ACF plot of the studentized residuals. Note that there is no differences between the simulation with or withput linear ionization trend as the former is merely a transformation of the latter.", fig.width=8, fig.height=4} 

gg.sim[[6]] 
```

This approach can be formalized with a *Ljung-Box Q test*, where the $H0$ is a white noise series (i.e. no autocorrelation). This supports the last of the three assumptions the residuals appear to be truly independent (*independence of residuals*) from one to the next observation.


#### Influence of variance on the regression model

Given that the "ideal" simulation satifies the conditions as initially set for the linear model Eq. \@ref(eq:linmodR), the estimated $\hat{\epsilon}}$ can be used to assess deivations from this "ideal" linear model, and thereby help estabelish how representative the $\bar{R}$ is for the sampled spot. Deviations from this "ideal" linear model might indicate heterogeneity of the analytical substrate or fluctuating stability of the analytical setup during the measurement. The goal of residual analysis would therefore be to identify the influence of the coefficient; in this case the influence on $\bar{R}$, by determining the leverage and discrepance (outlyingness) of variable $X^a$, such that:

\begin{equation}
  \text{Influence} = \text{Leverage} + \text{Discrepancy}
    (\#eq:influence)
\end{equation}

Having already determined the outlyingess, as the studentized residuals, and the leverage, as hat-values, of the indendent variable (with @eq:lev), it is possible to determine the influence of $X^a$ have on the linear model (Eq. \@ref(eq:influence)). This influence can be quantified with *Cook's Distance* (or *Cook's D*). This formulation overcomes the effect that high leverage does not necessarily require the data-point to be influential, as Cook's D combines the hat-values (@eq:lev) with studentized residuals (@eq:student.res), in a new equation:

\begin{equation}
  D_i = \frac{e_{i}^{*}\phantom{}^2}{k + 1} \times \frac{h_i}{1- h_i} 
  (\#eq:CD)
\end{equation}

, where the $k$ stands for the number of coefficients in the regression model. To quantify whether a $D_i$ is substantially larger then the rest of the sample a cut-off value is defined as 

\begin{equation}
  D_c =4/ (n-k-1)
  (\#eq:CDcut)
\end{equation}

This can also be visualized in a *Residuals vs Leverage plot* (Fig. \@ref(fig:cooksD)), where the red dots demarcating values with Cook's D values that might significantly influence the regression model [@Bruce2017]. If high numbers of high Cook's D values occur in an analysiss, this warrants precaution. This bivariate approach is more sensitive and can flag problematic analasys even if this is not obvious from the descriptive and predictive statistics alone (see following section).  

```{r cooksD, echo=FALSE,  fig.cap = "Residuals vs Leverage plot, comparing the  hat-values (leverage) with the studentized residuals (outlyingness). Red dots denote data-points that have Cook's D values (Eq. \\@ref(eq:CD)) that fall above the cut-off value (Eq. \\@ref(eq:CDcut))", fig.width=8, fig.height=4}

gg.sim[[5]] 
```


### A comparison of methods

<!-- The regression diagnostics of several of the analyses seem to indicate that internal precision of some of the analyses seems to be compromised. Regression diagnostic based count optimization can help detect anomolous.... -->
<!-- For a systematic evaluation of the results of the regression diagnostics, descriptive and predicted standard errors of the mean have been recalculated by omitting the count measurements that did not conform to normality (Fig. \@ref(fig:QQnorm)), and those values have demarcated with an asterisk ($e_\bar{x}^*$ and $\hat{e}_\bar{x}^*$) in Figures \@ref(fig:Cross), \@ref(fig:ResFit), \@ref(fig:ScLoc), \@ref(fig:QQnorm) and \@ref(fig:rslev). -->

<!-- To assess the effect of these adjustments between the analysis, the following two parameters are defined: -->

<!-- $$ \theta_{x_R} = \log{\left( \frac{e_{\bar{x}_{R}}}{e_{\bar{x}_{R}}^{*}}\right)} $$ {#eq:opt.t} -->

<!-- $$ \theta_{\hat{x}_R} = \log{\left( \frac{\hat{e}_{\bar{x}_{R}}}{\hat{e}_{\bar{x}_{R}}^{*}}\right)} $$ {#eq:opt.that} -->

<!-- , where  internal precision is compared for the augmented (based on *Cook's D*) and original datasets, by recalculating the descriptive and predictive statistics of the standard error of the mean. -->



<!-- Random number simulation is performed to asses systematic relationships between the predicted and descriptive measures on the augmented and original datasets counterparts (Fig. \@ref(fig:CDadj)). This approach suggest that there are defined trajectories based on analyte heterogeneity (varying $R$) and machine instability (reduced $N$). In all cases the actual precision ($e_{\bar{x}}$) is improved after omission of influential data-points (based on Cook's D). This could suggest that the error measured by the descriptive $e_{\bar{x}}$ can be entirely related to random nature of secondary ion generation in most cases, and as the $e_{\bar{x}}$ improves, the $\hat{e}_{\bar{x}}$ deteriorates because of reduced cumulative numbers of $N_i$. In the future these results will be used to monitor analyte heterogeneity (varying $R$) and machine instability (reduced $N$). -->


<!-- However when comparing large numbers of it can be shown that isotopically heteregenous follow specific trajectories upon comparison of predicted and descriptive statistics. In other words the amount of $N_i$ removed in specific domains will cause a case specific shift. -->




```{r gradient, echo=FALSE,  fig.cap = "Regression",  fig.width=12, fig.height=6}


sim_R.rep <- sim_R.ext %>% 
  filter(trend == "linear trend (var: 0)")

# comparison of methods
methods_vc <- c("Cameca", "CooksD") %>% set_names()

# Diagnostics
tb.dia <- purrr::map_dfr(methods_vc,
                         ~diag_R(sim_R.rep,
                                 method = .x,
                                 args = expr_R(Xt = "Xt.sim",
                                               N = "N.sim",
                                               species = "species",
                                               ion1 = "13C",
                                               ion2 = "12C"
                                               ),
                                 simulation,
                                 repetition,
                                 output = "flag"
                                 ),
                         .id = "methods_vc"
                        )

# Original
tb.R <- stat_R(sim_R.rep, 
               Xt.sim, 
               N.sim, 
               species, 
               ion1 = "13C",
               ion2 = "12C",
               simulation,
               repetition
               ) 

# Augmentation
tb.aug <- tb.dia %>%
  filter(flag == "non-influential") %>%
  stat_R(., 
         Xt.sim, 
         N.sim, 
         species, 
         ion1 = "13C",
         ion2 = "12C",
         methods_vc,
         simulation,
         repetition
        ) 

tb.trj <- left_join(tb.aug, 
                    tb.R, 
                    by = c("simulation", "repetition"), 
                    suffix = c(".aug", ".org")
                    ) 






diff_0_CAM <- filter(tb.trj, methods_vc == "Cameca", simulation == "ideal") %>% 
  mutate(Chi2 = (RS_R_Xt.sim.org - RS_R_Xt.sim.aug)^2 ) %>% 
  pull(Chi2) %>% 
  mean()


diff_0_CD <- filter(tb.trj, methods_vc == "CooksD", simulation == "ideal") %>% 
  mutate(Chi2 = (RS_R_Xt.sim.org - RS_R_Xt.sim.aug)^2 ) %>% 
  pull(Chi2) %>% 
  mean()


fun_chi <- function(x, mod0)  (x - mod0)^2 / x

tb.trj <- tb.trj %>% 
  group_by(methods_vc, simulation) %>% 
  mutate(diff_n  = (n_R_Xt.sim.org - n_R_Xt.sim.aug)^2,
         diff_RS = (RS_R_Xt.sim.org - RS_R_Xt.sim.aug)^2,
# proportional variance explained by augmentation
         prop_Rexp = RS_R_Xt.sim.aug / RS_R_Xt.sim.org,
         Chi_score = if_else(simulation == "Cameca", fun_chi(diff_RS, diff_0_CAM), fun_chi(diff_RS, diff_0_CD)),
         Chi2 = sum(Chi_score)
   
         ) %>% 
 ungroup()

```



```{r echo=FALSE}

ggplot(tb.trj , aes(x = diff_RS, y = diff_n)) +
  stat_density_2d(geom = "polygon", 
                  aes(alpha = stat(level), 
                      fill = simulation), 
                  color = "grey") +
  geom_point(shape = 3) +
  scale_alpha(guide = "none")+
  facet_grid(cols = vars(methods_vc))+
  ggtitle("simulations")

```


```{r echo=FALSE}

df <- reps
upper <- 600
# curve
df_den <- tibble::tibble(vec_den = seq(0.1, upper))

#find upper  95% of distribution
upper95 <- qchisq(.95, df)

sm <- distinct(tb.trj, methods_vc, simulation, .keep_all = TRUE)

ggplot(df_den, aes(x=vec_den)) +
  stat_function(fun = dchisq, args = list(df = df)) +
  stat_function(fun = dchisq, 
                      args = list(df = df),
                      xlim = c(upper95, upper),
                      geom = "area",
                      alpha = 0.2) +
  geom_vline(xintercept = upper95 , linetype = 2)+
  geom_point(data = sm, 
             aes(x = Chi2, y = dchisq(Chi2, df = df),
             color = simulation
             )
             ) +
  annotate("text", label = "95% confidence limit", x = upper95 * 1.2, 
           y = 0.05, angle = 90
           ) +
  xlab(expression(chi^2~"(df = "*df*")")) +
  ylab("probability-density") +
  facet_grid(cols = vars(methods_vc))

```


# References
